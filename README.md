# âœ‹ğŸ›ï¸ Hand Gesture-Based Volume Control

## ğŸŒŸ Overview âœ¨
The **Hand Gesture-Based Volume Control** project enables users to ğŸšï¸ control or adjust the system volume using simple ğŸ¤Œ hand gestures. By leveraging **ğŸ¤– MediaPipe** for hand tracking and **ğŸ”Š PyCaw** for audio control, this project showcases an intuitive, touch-free way to manage ğŸ”ˆ volume levels. The program ğŸ“¹ captures hand movements through a ğŸ“· webcam and maps the distance between specific âœ‹ hand landmarks to adjust the ğŸ¶ volume.

---

## ğŸ› ï¸ Features ğŸ”§ğŸŒŸ
- **ğŸ™Œ Touch-Free Volume Control**: Adjust the ğŸ”‰ volume of your system by simply moving your ğŸ¤ fingers.
- **â±ï¸ Real-Time Hand Tracking**: Tracks âœ‹ hand gestures with high accuracy using ğŸ¤– MediaPipe.
- **ğŸ“Š Interactive Visual Feedback**: Displays volume percentage ğŸ”¢ and real-time ğŸ¥ visual indicators for user interaction.
- **âš¡ Performance Metrics**: Displays ğŸ•’ FPS (frames per second) to monitor application performance.

---

## ğŸ–¥ï¸ Technologies Used ğŸ’»ğŸ”ğŸ› ï¸
- **ğŸ Python**: Programming language used for development.
- **ğŸ¤– MediaPipe**: For âœ‹ hand landmark detection and tracking.
- **ğŸ”Š PyCaw**: For accessing and controlling system ğŸµ audio endpoints.
- **ğŸ“· OpenCV**: For ğŸ“¹ webcam access and ğŸ–¼ï¸ image processing.
- **ğŸ“ NumPy**: For mathematical operations like ğŸ“‰ interpolation.

---

## ğŸ“¥ Installation ğŸ“¦âš™ï¸
1. ğŸ§ª Clone this repository:
   ```bash
   git clone https://github.com/Tejasva-Maurya/Hand-Gesture-Volume-Control.git
   cd Hand-Gesture-Volume-Control
   ```
2. ğŸ“¦ Install the required dependencies:
   ```bash
   pip install opencv-python mediapipe pycaw numpy comtypes
   ```
3. â–¶ï¸ Run the program:
   ```bash
   python main.py
   ```

---

## ğŸ› ï¸ How It Works ğŸ§ ğŸ¤¹â€â™‚ï¸

1. **ğŸ“· Webcam Input**: The program captures ğŸ“¹ video frames using OpenCV.
2. **âœ‹ Hand Tracking**: ğŸ¤– MediaPipe detects and tracks hand ğŸ–– landmarks in the video stream.
3. **ğŸ¤ Gesture Detection**:
   - Detects the position of the ğŸ‘ thumb and ğŸ–• index finger.
   - Calculates the distance between these two fingers.
   - Interpolates the distance to map it to a ğŸ”ˆ volume range.
4. **ğŸšï¸ Volume Control**:
   - Uses ğŸ”Š PyCaw to adjust the system ğŸ”ˆ volume based on the calculated distance.
5. **ğŸ¥ Visual Feedback**:
   - Displays a ğŸ“ rectangle representing the current ğŸ”Š volume level.
   - Shows ğŸ•’ FPS and other interactive ğŸ¨ elements for user feedback.

---

## ğŸ§© Code Explanation ğŸ“ğŸ”
### ğŸ§± Key Components ğŸ”‘ğŸ› ï¸

#### 1. **âœ‹ Hand Tracking**
- `detector = htm.handDetector(maxhands=1, detectionCon=0.8, trackingCon=0.8)`
  - Initializes the âœ‹ hand detection module with parameters for ğŸ¯ accuracy and tracking ğŸ”— confidence.
- `lmList = detector.findPosition(img, draw=False)`
  - Extracts the list of âœ‹ hand landmarks detected in the ğŸ¥ frame.

#### 2. **ğŸ”¢ Volume Calculation**
- `length = math.hypot(x2 - x1, y2 - y1)`
  - Calculates the ğŸ“ Euclidean distance between the ğŸ‘ thumb and ğŸ–• index finger.
- `vol = np.interp(length, [30, 300], [minVol, maxVol])`
  - Maps the ğŸ¤Œ finger distance to the system ğŸ”ˆ volume range using ğŸ“‰ interpolation.

#### 3. **ğŸ›ï¸ Volume Adjustment**
- `volume.SetMasterVolumeLevel(vol, None)`
  - Adjusts the system ğŸ¶ volume using ğŸ”Š PyCaw.

#### 4. **ğŸ“Š Visual Feedback**
- Displays the ğŸšï¸ volume bar and percentage ğŸ”¢ on the ğŸ–¥ï¸ screen.
- Highlights the interaction point between ğŸ¤ fingers with ğŸ¨ colored circles.

---

## ğŸ’¡ Use Cases ğŸ”ğŸ’¡
- **â™¿ Accessibility**: Provides an alternative input method for individuals with limited mobility.
- **ğŸ§¼ Touch-Free Interaction**: Useful in scenarios where physical interaction with devices is inconvenient or unhygienic.
- **ğŸ® Gaming and Multimedia**: Offers a modern, intuitive way to control ğŸ”Š audio during ğŸ® gaming or media playback.

---

## ğŸš€ Future Potential ğŸ”®ğŸš€
1. **ğŸ™Œ Multi-Gesture Support**:
   - Implement additional gestures for functionalities like â–¶ï¸ play, â¸ï¸ pause, or ğŸŒ brightness adjustment.
2. **ğŸ™ï¸ Voice Command Integration**:
   - Combine with ğŸ¤ voice commands for a multimodal ğŸ›ï¸ interaction system.
3. **ğŸ’» Cross-Platform Compatibility**:
   - Extend support to ğŸ–¥ï¸ macOS and ğŸ§ Linux systems.
4. **ğŸ” Advanced Tracking**:
   - Use depth sensors for more accurate âœ‹ hand tracking in ğŸŒ‘ low-light conditions.
5. **ğŸ“ Customizable Gesture Library**:
   - Allow users to define and train their own âœ‹ gestures.

---

## âš ï¸ Limitations âš¡âš ï¸
- Relies on adequate ğŸŒ lighting for accurate âœ‹ hand tracking.
- Requires a high-quality ğŸ“· webcam for optimal performance.
- May not work seamlessly in environments with ğŸ¨ complex backgrounds.

---

## ğŸ“œ License âš–ï¸ğŸ“„ğŸ“
This project is licensed under the MIT License. Feel free to use, modify, and distribute the software according to the terms of this license.

---

## ğŸ™ Acknowledgments ğŸ™ŒğŸ“šğŸ“
- [ğŸ¤– MediaPipe Documentation](https://ai.google.dev/edge/mediapipe/solutions/guide)
- [ğŸ”Š PyCaw Documentation](https://github.com/AndreMiras/pycaw)
- ğŸ“· OpenCV and ğŸ Python communities for extensive support and resources.

